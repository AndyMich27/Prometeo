{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "07_NormasInducidasAplicacion.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyODjTEFUMI6uxybP0mXz+cy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jugernaut/Prometeo/blob/desarrollo/02_AlgebraLineal/06_Normas/07_NormasInducidasAplicaciones.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKcKm7OJbfaC"
      },
      "source": [
        "##Normas Inducidas Aplicación\n",
        "### Proyecto PAPIME PE101019\n",
        "- Autor: Miguel Angel Pérez León\n",
        "- Rev: mar oct  1 17:08:27 CDT 2020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pL3l51BpO_hL"
      },
      "source": [
        "## Introducción\n",
        "\n",
        "El concepto de **norma** es de gran importancia en el contexto del álgebra linea, tiene multiples aplicaciones y una de ellas es para definir metricas dentro de los espacios vectoriales y así para poder medir distancias entre los elementos que pertenecen a estos espacios vectoriales.\n",
        "\n",
        "La **distancia** entre un vector y el origen o entre una matriz y otra matriz puede tener multiples interpretaciones, en este documento veremos como el concepto de norma se aplica al campo de la inteligencia artificial y en particular en las redes neuronales.\n",
        "\n",
        "Las redes neuronales y el aprendizaje de máquina son de las áreas de las ciencias que han tenido más desarrollo en las últimas epocas.\n",
        "\n",
        "Y una de las áreas en las que mayor aplicación han tenido estas disciplinas es en la **clasificicación automatizada**.\n",
        "\n",
        "Supongamos que nos interesa clasificar colores y dado que los colores se pueden ver como vectores con 3 componentes entonces podemos establecer distancias entre estos vectores para determinar que tan \"similares\" o \"distintos\" son unos de otros.\n",
        "\n",
        "A partir de este concepto de \"distancia\" (norma en un espacio vectorial) podemos comenzar a clasificar cualquier cosa que pueda ser caracterizada por un vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85Aws8yJGKXm",
        "outputId": "9410abaa-85ba-495a-b618-ab9d477c0635",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        }
      },
      "source": [
        "!pip install tensorflow==1.1\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "#%tensorflow_version 1.x\n",
        " \n",
        "class SOM(object):\n",
        "    \"\"\"\n",
        "    Clase que representa una red neuronal tipo SOM.\n",
        "    \"\"\"\n",
        " \n",
        "    #To check if the SOM has been trained\n",
        "    _trained = False\n",
        " \n",
        "    def __init__(self, m, n, dim, n_iterations=100, alpha=None, sigma=None):\n",
        "        \"\"\"\n",
        "        Constructor que toma como parametros los valores descritos en el\n",
        "        algoritmo SOM. Genera un mapa de m renglones por n columnas y se entrenara\n",
        "        con n_iterations\n",
        "        \"\"\"\n",
        " \n",
        "        #Se inicializan variables que seran usadas a lo largo del coidgo\n",
        "        self._m = m\n",
        "        self._n = n\n",
        "        if alpha is None:\n",
        "            alpha = 0.3\n",
        "        else:\n",
        "            alpha = float(alpha)\n",
        "        if sigma is None:\n",
        "            sigma = max(m, n) / 2.0\n",
        "        else:\n",
        "            sigma = float(sigma)\n",
        "        self._n_iterations = abs(int(n_iterations))\n",
        " \n",
        "        '''SE NECESITA UNA GRAFICA (PLANO), hay una grafica\n",
        "        por default pero la guardamos en _graph'''\n",
        "        self._graph = tf.Graph()\n",
        " \n",
        "        '''SE CREAN LOS ELEMENTOS NECESARIOS EN LA GRAFICA'''\n",
        "        with self._graph.as_default():\n",
        "            '''SE CREAN TODAS LAS NEURONAS CON tf.Variable, son m*n\n",
        "            neuronas con dim pesos, que seran comparados con los pesos\n",
        "            de la entrada y la que tenga la menor distancia sera la\n",
        "            neurona ganadora. Antes de iniciar el entrenamiento, hay\n",
        "            hay que inicializar TODAS las variables'''\n",
        "            \n",
        "            '''Lista de pesos de los vectores de la red neuronal'''\n",
        "            self._weightage_vects = tf.Variable(tf.random_normal(\n",
        "                [m*n, dim]))\n",
        " \n",
        "            '''Lista de 600 entradas, y cada entrada representa una\n",
        "            coordenada en la cual se encuentra cada neurona'''\n",
        "            self._location_vects = tf.constant(np.array(\n",
        "                list(self._neuron_locations(m, n))))\n",
        " \n",
        "            '''self._vect_input es un placeholder de tamano dim, ya que\n",
        "            es el objeto que sera alimentado con el vector de entrada y\n",
        "            a su vez este sera comparado con los pesos de cada neurona.\n",
        "            Esto es asi por el framework que da tensorflow'''\n",
        "            self._vect_input = tf.placeholder(\"float\", [dim])\n",
        "            \n",
        "            '''Lo mismo sucede con esta variable, la diferencia es que en\n",
        "            este punto aun no se sabe cuantas iteraciones (epocas) seran\n",
        "            necesarias, asi que se deja en cero.'''\n",
        "            self._iter_input = tf.placeholder(\"float\")\n",
        " \n",
        "            '''Devuelve el indice con el menor valor, es decir la neurona mas cercana.'''\n",
        "            bmu_index = tf.argmin(tf.sqrt(tf.reduce_sum(\n",
        "                tf.pow(tf.subtract(self._weightage_vects, tf.stack(\n",
        "                    [self._vect_input for i in range(m*n)])), 2), 1)),\n",
        "                                  0)\n",
        " \n",
        "            '''Variable que guarda el indice y un espacio para el sus\n",
        "            coordenada'''\n",
        "            slice_input = tf.pad(tf.reshape(bmu_index, [1]),\n",
        "                                 np.array([[0, 1]]))\n",
        "            bmu_loc = tf.reshape(tf.slice(self._location_vects, slice_input,\n",
        "                                          tf.constant(np.array([1, 2]))),\n",
        "                                 [2])\n",
        " \n",
        "            '''Valores necesario para actualizar los pesos de las neuronas\n",
        "            de acuerdo a la iteracion (epoca)'''\n",
        "            learning_rate_op = tf.subtract(1.0, tf.div(self._iter_input,\n",
        "                                                  self._n_iterations))\n",
        "            _alpha_op = tf.multiply(alpha, learning_rate_op)\n",
        "            _sigma_op = tf.multiply(sigma, learning_rate_op)\n",
        " \n",
        "            '''Calcula las distancias al cuadrado por cada neurona con respecto\n",
        "            a la neurona GANADORA (BMU). De tal manera que estos valores\n",
        "            puedan ser empleados para actualizar los pesos de los vecinos'''\n",
        "            bmu_distance_squares = tf.reduce_sum(tf.pow(tf.subtract(\n",
        "                self._location_vects, tf.stack(\n",
        "                    [bmu_loc for i in range(m*n)])), 2), 1)\n",
        "            neighbourhood_func = tf.exp(tf.negative(tf.div(tf.cast(\n",
        "                bmu_distance_squares, \"float32\"), tf.pow(_sigma_op, 2))))\n",
        "            learning_rate_op = tf.multiply(_alpha_op, neighbourhood_func)\n",
        " \n",
        "            '''Tasa de aprendizaje para actualizar los pesos de las neuronas'''\n",
        "            learning_rate_multiplier = tf.stack([tf.tile(tf.slice(\n",
        "                learning_rate_op, np.array([i]), np.array([1])), [dim])\n",
        "                                               for i in range(m*n)])\n",
        "            weightage_delta = tf.multiply(\n",
        "                learning_rate_multiplier,\n",
        "                tf.subtract(tf.stack([self._vect_input for i in range(m*n)]),\n",
        "                       self._weightage_vects)) \n",
        "            \n",
        "            '''Actualiza todos los pesos de las neuronas de acuerdo a los\n",
        "            parametros calculados previamente'''                                        \n",
        "            new_weightages_op = tf.add(self._weightage_vects,\n",
        "                                       weightage_delta)\n",
        "            \n",
        "            '''Se guarda la ultima operacion realizada en la SOM, ya que\n",
        "            esta operacion sera la que se ejecute y a su vez ejecuta todas\n",
        "            las operaciones previar al llamar a sess.run()'''\n",
        "            self._training_op = tf.assign(self._weightage_vects,\n",
        "                                          new_weightages_op)                                       \n",
        " \n",
        "            '''En tensorflow todo debe ocurrir dentro de una sesion, es por\n",
        "            este motivo que se guarda la sesion'''\n",
        "            self._sess = tf.Session()\n",
        " \n",
        "            '''Forma en la tensorflow inicializa sus variables antes de ser\n",
        "            utilizadas'''\n",
        "            init_op = tf.initialize_all_variables()\n",
        "            self._sess.run(init_op)\n",
        "            \n",
        "            '''centroid_grid es un mapa de bits en el cual se guardan los\n",
        "            valores de las neuronas. Es de tamano m, por que para cada renglon\n",
        "            se tienen n neuronas y sus respectivos valores. '''\n",
        "            centroid_grid = [[] for i in range(self._m)]\n",
        "            self._weightages = list(self._sess.run(self._weightage_vects))\n",
        "            self._locations = list(self._sess.run(self._location_vects))\n",
        "    \n",
        "            '''Con este for, se accede a cada neurona por posicion y se guarda\n",
        "            en centroid_grid sus pesos. El resultado es un mapa de bits que puede\n",
        "            ser facilmente graficado por matplotlib. Es el mapa incial (SIN ENTRENAR)'''\n",
        "            for i, loc in enumerate(self._locations):\n",
        "                centroid_grid[loc[0]].append(self._weightages[i])\n",
        "            self._mapa_inicial = centroid_grid\n",
        " \n",
        "    def _neuron_locations(self, m, n):\n",
        "        '''Yield regresa un generador flojo, y hasta que es necesario\n",
        "        se evalua. Esto se hace para que no haya informacion no necesaria\n",
        "        en memoria. En el constructor el resultado de esta funcion se\n",
        "        mete en una lista para que sea accesible de inmediato'''\n",
        "        for i in range(m):\n",
        "            for j in range(n):\n",
        "                yield np.array([i, j])\n",
        " \n",
        "    def train(self, input_vects):\n",
        "        '''Para cada iteracion (epoca) se realiza el entrenamiento'''\n",
        "        for iter_no in range(self._n_iterations):\n",
        "            #Train with each vector one by one\n",
        "            for input_vect in input_vects:\n",
        "                self._sess.run(self._training_op,\n",
        "                               feed_dict={self._vect_input: input_vect,\n",
        "                                          self._iter_input: iter_no})\n",
        " \n",
        "        '''centroid_grid es un mapa de bits en el cual se guardan los\n",
        "            valores de las neuronas. Es de tamano m, por que para cada renglon\n",
        "            se tienen n neuronas y sus respectivos valores. '''\n",
        "        centroid_grid = [[] for i in range(self._m)]\n",
        "        self._weightages = list(self._sess.run(self._weightage_vects))\n",
        "        self._locations = list(self._sess.run(self._location_vects))\n",
        "        \n",
        "        '''Con este for, se accede a cada neurona por posicion y se guarda\n",
        "            en centroid_grid sus pesos. El resultado es un mapa de bits que puede\n",
        "            ser facilmente graficado por matplotlib. En este punto la red ya esta entrenada.'''\n",
        "        for i, loc in enumerate(self._locations):\n",
        "            centroid_grid[loc[0]].append(self._weightages[i])\n",
        "        self._centroid_grid = centroid_grid\n",
        " \n",
        "        '''En este punto la red ya esta entrenada.'''\n",
        "        self._trained = True\n",
        " \n",
        "    def get_centroids(self):\n",
        "        # Solo devuelve los centroides para que puendan ser graficados\n",
        "        if not self._trained:\n",
        "            raise ValueError(\"La red aun no ha sido entrenada\")\n",
        "        return self._centroid_grid\n",
        " \n",
        "    def map_vects(self, input_vects):\n",
        "        '''to_return es la lista que contiene las coordenadas (x,y) de la\n",
        "        neurona que mas se parece a cada una de las entradas de input_vects\n",
        "        en el mismo orden'''\n",
        " \n",
        "        if not self._trained:\n",
        "            raise ValueError(\"SOM not trained yet\")\n",
        " \n",
        "        to_return = []\n",
        "        for vect in input_vects:\n",
        "            min_index = min([i for i in range(len(self._weightages))],\n",
        "                            key=lambda x: np.linalg.norm(vect-\n",
        "                                                         self._weightages[x]))\n",
        "            to_return.append(self._locations[min_index])\n",
        " \n",
        "        return to_return\n",
        "    \n",
        "    def map_vect(self, vect):\n",
        "        '''\n",
        "        Mapea un solo vector y devuelve la clasificacion vista como\n",
        "        un indice relacionado a la coordenada (x,y) de la neurona\n",
        "        '''\n",
        "\n",
        "        min_index = min([i for i in range(len(self._weightages))],\n",
        "                        key=lambda x: np.linalg.norm(\n",
        "                            vect - self._weightages[x]))\n",
        "        pos2D = self._locations[min_index]\n",
        "        # polinomio de direccionamiento de la neurona\n",
        "        #return pos2D[0]*self._m + pos2D[1], pos2D\n",
        "        return (pos2D[0], pos2D[1])\n",
        " \n",
        "# Vectores de entrenamiento RGBcolors\n",
        "colors = np.array(\n",
        "     [[0., 0., 0.],\n",
        "      [0., 0., 1.],\n",
        "      [0., 0., 0.5],\n",
        "      [0.125, 0.529, 1.0],\n",
        "      [0.33, 0.4, 0.67],\n",
        "      [0.6, 0.5, 1.0],\n",
        "      [0., 1., 0.],\n",
        "      [1., 0., 0.],\n",
        "      [0., 1., 1.],\n",
        "      [1., 0., 1.],\n",
        "      [1., 1., 0.],\n",
        "      [1., 1., 1.],\n",
        "      [.33, .33, .33],\n",
        "      [.5, .5, .5],\n",
        "      [.66, .66, .66]])\n",
        "color_names = \\\n",
        "    ['negro', 'azul', 'azul marino', 'azul cielo',\n",
        "     'gris azulado', 'lila', 'verde', 'rojo',\n",
        "     'cyan', 'violeta', 'amarillo', 'blanco',\n",
        "     'gris obscuro', 'gris medio', 'gris claro']\n",
        " \n",
        "# Creamos un SOM de 20x30 y se entrena 00 veces\n",
        "som = SOM(20, 30, 3, 400)\n",
        "\n",
        "# Se muestra el mapa inicial\n",
        "mapa_inicial = som._mapa_inicial\n",
        "plt.imshow(mapa_inicial)\n",
        "plt.title('Red Neuronal Inicial')\n",
        "plt.show()\n",
        "\n",
        "# Se entrena la red con un conjunto de colores\n",
        "som.train(colors)\n",
        " \n",
        "# Obtenemos el SOM ya entrenado\n",
        "image_grid = som.get_centroids()\n",
        " \n",
        "# Contiene la lista de coordenadas de los correspondientes colores\n",
        "mapped = som.map_vects(colors)\n",
        " \n",
        "# Grafica\n",
        "plt.imshow(image_grid)\n",
        "plt.title('Red Neuronal Entrenada')\n",
        "for i, m in enumerate(mapped):\n",
        "    plt.text(m[1], m[0], color_names[i], ha='center', va='center',\n",
        "             bbox=dict(facecolor='white', alpha=0.5, lw=0))\n",
        "plt.show()\n",
        "\n",
        "# Se le muestra un color para que indique a que neurona se parece mas\n",
        "print(som.map_vect([0.125, 0.529, 1.0]))\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/e4/b2a8bcd1fa689489050386ec70c5c547e4a75d06f2cc2b55f45463cd092c/tensorflow-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.4MB)\n",
            "\u001b[K     |████████████████████████████████| 31.4MB 146kB/s \n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1) (0.35.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorflow==1.1) (50.3.0)\n",
            "Installing collected packages: tensorflow\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed tensorflow-1.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-546fd73dea33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;31m# Creamos un SOM de 20x30 y se entrena 00 veces\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m \u001b[0msom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSOM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;31m# Se muestra el mapa inicial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-546fd73dea33>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, m, n, dim, n_iterations, alpha, sigma)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;34m'''Lista de pesos de los vectores de la red neuronal'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             self._weightage_vects = tf.Variable(tf.random_normal(\n\u001b[0m\u001b[1;32m     49\u001b[0m                 [m*n, dim]))\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'random_normal'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHRqaGPtnHza"
      },
      "source": [
        "De igual manera como se definió la norma en el espacio de los vectores en $\\mathbb{R}^{n}$, en esta sección se procede a definir la norma para el espacio de las matrices $A\\in M_{n\\times n}$ sobre $\\mathbb{R}$.\n",
        "\n",
        "**Una función** $\\left\\Vert \\cdot\\right\\Vert$ de matrices, se denomina norma matricial si $\\left\\Vert \\cdot\\right\\Vert$  si para cualesquiera matrices $A, B$ de $n\\times n$ se satisfacen las siguientes propiedades.\n",
        "\n",
        "1.- $\\left\\Vert A\\right\\Vert \\geq0$.\\\n",
        "2.- $\\left\\Vert A\\right\\Vert =0\\,\\Longleftrightarrow A=0$.\\\n",
        "3.- $\\left\\Vert \\alpha A\\right\\Vert =\\left|\\alpha\\right|\\left\\Vert A\\right\\Vert$.\\\n",
        "4.- $\\left\\Vert A+B\\right\\Vert \\leq\\left\\Vert A\\right\\Vert +\\left\\Vert B\\right\\Vert$. (desigualdad triangular)\\\n",
        "5.- $\\left\\Vert AB\\right\\Vert \\leq\\left\\Vert A\\right\\Vert \\left\\Vert B\\right\\Vert$ (compatibilidad o consistencia).\n",
        "\n",
        "¿Conoces alguna función $\\left(\\left\\Vert \\cdot\\right\\Vert \\right)$ que cumpla con las propiedades anteriores para el espacio de las matrices de $n\\times n$?. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYjY41_oGwmq",
        "outputId": "0b34547c-b568-4a64-9c0b-b9a2b2c212c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "from SOM import SOM\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        " \n",
        "# Vectores de entrenamiento RGBcolors\n",
        "colors = np.array(\n",
        "     [[0., 0., 0.],\n",
        "      [0., 0., 1.],\n",
        "      [0., 0., 0.5],\n",
        "      [0.125, 0.529, 1.0],\n",
        "      [0.33, 0.4, 0.67],\n",
        "      [0.6, 0.5, 1.0],\n",
        "      [0., 1., 0.],\n",
        "      [1., 0., 0.],\n",
        "      [0., 1., 1.],\n",
        "      [1., 0., 1.],\n",
        "      [1., 1., 0.],\n",
        "      [1., 1., 1.],\n",
        "      [.33, .33, .33],\n",
        "      [.5, .5, .5],\n",
        "      [.66, .66, .66]])\n",
        "color_names = \\\n",
        "    ['negro', 'azul', 'azul marino', 'azul cielo',\n",
        "     'gris azulado', 'lila', 'verde', 'rojo',\n",
        "     'cyan', 'violeta', 'amarillo', 'blanco',\n",
        "     'gris obscuro', 'gris medio', 'gris claro']\n",
        " \n",
        "# Creamos un SOM de 20x30 y se entrena 00 veces\n",
        "som = SOM(20, 30, 3, 400)\n",
        "\n",
        "# Se muestra el mapa inicial\n",
        "mapa_inicial = som._mapa_inicial\n",
        "plt.imshow(mapa_inicial)\n",
        "plt.title('Red Neuronal Inicial')\n",
        "plt.show()\n",
        "\n",
        "# Se entrena la red con un conjunto de colores\n",
        "som.train(colors)\n",
        " \n",
        "# Obtenemos el SOM ya entrenado\n",
        "image_grid = som.get_centroids()\n",
        " \n",
        "# Contiene la lista de coordenadas de los correspondientes colores\n",
        "mapped = som.map_vects(colors)\n",
        " \n",
        "# Grafica\n",
        "plt.imshow(image_grid)\n",
        "plt.title('Red Neuronal Entrenada')\n",
        "for i, m in enumerate(mapped):\n",
        "    plt.text(m[1], m[0], color_names[i], ha='center', va='center',\n",
        "             bbox=dict(facecolor='white', alpha=0.5, lw=0))\n",
        "plt.show()\n",
        "\n",
        "# Se le muestra un color para que indique a que neurona se parece mas\n",
        "print som.map_vect([0.125, 0.529, 1.0])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-bfb561ecbe5c>\"\u001b[0;36m, line \u001b[0;32m54\u001b[0m\n\u001b[0;31m    print som.map_vect([0.125, 0.529, 1.0])\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUR6fBxlqMgC"
      },
      "source": [
        "## Mapa auto organizado (SOM)\n",
        "\n",
        "Un mapa auto organizado o SOM por sus siglas en inglés (self-organized map) es una de las redes neuronales más sencilla y facile de implementar pero no por eso es un algoritmo que que no tenga aplicación actualmente.\n",
        "\n",
        "Este tipo de red neuronal fue creado en la decada de los 80's por el por el finlandés Teuvo Kohonen y se basa en modelos matemáticos de Alan Turing.\n",
        "\n",
        "La idea detras de este algoritmo es muy sencilla y se describe de manera breve a continuación:\n",
        "\n",
        "*   Comenzamos con una red o mapa (matriz) de vectores o incluso de matrices en la cual todas las neuronas o entrades del mapa contienen valores aleatorios.\n",
        "*   Elemento de la lista\n",
        "*   Elemento de la lista\n",
        "*   Elemento de la lista\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQdgCiNDrBRg"
      },
      "source": [
        "## 1-Norma Matricial\n",
        "\n",
        "Hecho. Recordatorio de álgebra lineal\n",
        "\n",
        "Hecho. Sea $A=\\left(\\begin{array}{cc}\n",
        "a_{11} & a_{12}\\\\\n",
        "a_{21} & a_{22}\n",
        "\\end{array}\\right)$ y $\\vec{x}=\\left(\\begin{array}{c}\n",
        "x_{1}\\\\\n",
        "x_{2}\n",
        "\\end{array}\\right)$ la operación $A\\vec{x}$ esta definida como$$A\\vec{x}=\\left(\\begin{array}{c}\n",
        "a_{11}x_{1}+a_{12}x_{2}\\\\\n",
        "a_{21}x_{1}+a_{22}x_{2}\n",
        "\\end{array}\\right)=\\left(\\begin{array}{c}\n",
        "c_{1}\\\\\n",
        "c_{2}\n",
        "\\end{array}\\right)$$\n",
        "\n",
        "De tal manera que cada entrada $c_{i}$ la podemos ver como una sumatoria, es decir\n",
        "\n",
        "$$\\begin{eqnarray*}\n",
        "c_{1}=\\sum_{i=1}^{n} a_{1j}x_{j}\\\\\n",
        "c_{2}=\\sum_{j=1}^{2} a_{2j}x_{j}\\end{eqnarray*}\\tag{1}$$\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2t_BRrcZMhm1"
      },
      "source": [
        "## 1-Norma Matricial\n",
        "\n",
        "Para poder demostrar que la 1-norma matricial de $A\\in M_{n\\times n}$ es el máximo de las normas de los vectores columnas es decir, $$\\left\\Vert A\\right\\Vert _{1}=\\underset{1\\leq j\\leq n}{max}\\left\\Vert \\vec{a}_{j}\\right\\Vert _{1}\\tag{2}$$\n",
        "\n",
        "Se necesita considerar lo mostrado en la celda anterior y pensar en la matriz A como una lista de columnas.\n",
        "\n",
        "Prueba de (2)\n",
        "\n",
        "Sea $\\vec{x}\\in\\mathbb{R}^{n}$ y $A\\in M_{n\\times n}$ con $A=\\left[\\vec{a}_{1}|\\vec{a}_{2}\\text{|}\\cdots\\text{|}\\vec{a}_{n}\\right]$, donde $\\vec{a}_{j}$ es un vector columna en $\\mathbb{R}^{n}$. Consideremos la bola unitaria en la 1-norma $:=\\left\\{ \\vec{x}\\in\\mathbb{R}^{n}\\mid\\left\\Vert \\vec{x}\\right\\Vert _{1}=\\sum_{j=1}^{n}\\left|x_{j}\\right|\\leq1\\right\\}$ . Cualquier vector $A\\vec{x}$ satisface:\n",
        "\n",
        "$$\\left\\Vert A\\vec{x}\\right\\Vert _{1}\t\\overset{\\underbrace{(1)}}{=}\t\\left\\Vert \\sum_{j=1}^{n}\\vec{a}_{j}x_{j}\\right\\Vert _{1}\\overset{\\underbrace{prop\\,4\\,nor}}{\\leq}\\sum_{j=1}^{n}\\left\\Vert \\vec{a}_{j}x_{j}\\right\\Vert _{1}\\overset{\\underbrace{prop\\,3\\,nor.}}{=}\\sum_{j=1}^{n}\\left\\Vert \\vec{a}_{j}\\right\\Vert _{1}\\left|x_{j}\\right|\n",
        "\t\\overset{\\underbrace{def.max.}}{\\leq}\t\\underset{1\\leq j\\leq n}{max}\\left\\Vert \\vec{a}_{j}\\right\\Vert _{1}\\sum_{j=1}^{n}\\left|x_{j}\\right|\\overset{\\underbrace{bola\\,unit.}}{\\leq}\\underset{1\\leq j\\leq n}{max}\\left\\Vert \\vec{a}_{j}\\right\\Vert _{1}\n",
        "\\Longrightarrow\t\\left\\Vert A\\vec{x}\\right\\Vert _{1}\t\\leq\\underset{1\\leq j\\leq n}{max}\\left\\Vert \\vec{a}_{j}\\right\\Vert _{1}$$\n",
        "\n",
        "Prueba. Si elegimos $\\vec{x}=\\vec{e}_{j}$ (es decir el vector canónico, ademas $\\left\\Vert \\vec{e}_{j}\\right\\Vert _{1}=1)$ con un 1 en la entrada j. Donde j maximiza, $\\left\\Vert \\vec{a}_{j}\\right\\Vert _{1}$, obtenemos la cota máxima.$$\\therefore\\left\\Vert A\\right\\Vert _{1}=\\underset{x\\neq0}{sup}\\frac{\\left\\Vert A\\vec{x}\\right\\Vert _{1}}{\\left\\Vert \\vec{x}\\right\\Vert _{1}}=\\underset{1\\leq j\\leq n}{max}\\left\\Vert \\vec{a}_{j}\\right\\Vert _{1}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlPJ7c1dRe5D"
      },
      "source": [
        "## Ejemplo 1-norma y del supremo. Sea $$A=\\left(\\begin{array}{ccc}\n",
        "3 & -1 & 4\\\\\n",
        "-5 & 0 & 2\\\\\n",
        "1 & -2 & 6\n",
        "\\end{array}\\right)$$\n",
        "\n",
        "Calcular $\\left\\Vert A\\right\\Vert _{1}$ y $\\left\\Vert A\\right\\Vert _{\\infty}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p_cXJ_Y9qVU",
        "outputId": "b37984ed-7051-414c-de2b-d368511162d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "A = np.array([[3, -1, 4],[-5, 0, 2],[1, -2, 6]])\n",
        "print (A)\n",
        "\n",
        "# Calculamos la 1-norma de A\n",
        "print (np.linalg.norm(A, 1))\n",
        "\n",
        "# Calculamos la norma del supremo de A\n",
        "print (np.linalg.norm(A, np.inf))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 3 -1  4]\n",
            " [-5  0  2]\n",
            " [ 1 -2  6]]\n",
            "12.0\n",
            "9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_Ym3FtKRP1t"
      },
      "source": [
        "## Ejemplo 2-norma o norma espectral\n",
        "\n",
        "**Definición.** La norma espectral se define de la siguiente manera, sea $A\\in M_{n\\times n}$ y $\\vec{x}\\in\\mathbb{R}^{n}.\\left\\Vert A\\right\\Vert _{2}=\\underset{x\\neq0}{sup}\\frac{\\left\\Vert A\\vec{x}\\right\\Vert _{2}}{\\left\\Vert \\vec{x}\\right\\Vert _{2}}$\n",
        "\n",
        "Aunque también se puede mostrar que. $\\left\\Vert A\\right\\Vert _{2}=\\sqrt{max\\,eigenvalor\\,A^{T}A}$\n",
        "\n",
        "Ejemplo. Sea $$A=\\left(\\begin{array}{cc}\n",
        "2 & 5\\\\\n",
        "1 & 3\n",
        "\\end{array}\\right)$$ Calcular $\\left\\Vert A\\right\\Vert _{2}$\n",
        "\n",
        "• Para encontrar los eigenvalores de A necesitamos calcular $det\\left(\\left(A^{T}A\\right)-\\lambda I\\right)$.\n",
        "\n",
        "• Lo primero es calcular la matriz transpuesta de $A$, es decir $$A^{T}=\\left(\\begin{array}{cc}\n",
        "2 & 1\\\\\n",
        "5 & 3\n",
        "\\end{array}\\right)$$\n",
        "\n",
        "• Ahora es necesario encontrar el producto de $A$ transpuesta por $A$.$$A^{T}A=\\left(\\begin{array}{cc}\n",
        "2 & 1\\\\\n",
        "5 & 3\n",
        "\\end{array}\\right)\\left(\\begin{array}{cc}\n",
        "2 & 5\\\\\n",
        "1 & 3\n",
        "\\end{array}\\right)=\\left(\\begin{array}{cc}\n",
        "5 & 13\\\\\n",
        "13 & 34\n",
        "\\end{array}\\right)$$\n",
        "\n",
        "• Lo siguiente es calcular el producto anterior menos $lambda-veces$ la identidad.$$A^{T}A-\\lambda I=\\left(\\begin{array}{cc}\n",
        "5 & 13\\\\\n",
        "13 & 34\n",
        "\\end{array}\\right)-\\left(\\begin{array}{cc}\n",
        "\\lambda & 0\\\\\n",
        "0 & \\lambda\n",
        "\\end{array}\\right)=\\left(\\begin{array}{cc}\n",
        "5-\\lambda & 13\\\\\n",
        "13 & 34-\\lambda\n",
        "\\end{array}\\right)$$\n",
        "\n",
        "• Finalmente se calcula el determinante de la matriz anterior \n",
        "$$\\begin{eqnarray*}det\\left(\\left(A^{T}A\\right)-\\lambda I\\right) =\t\\left(5-\\lambda\\right)\\left(34-\\lambda\\right)-\\left(13\\times13\\right) \\\\\n",
        "=\t170-5\\lambda-34\\lambda+\\lambda^{2}-169 \\\\\n",
        "=\t\\lambda^{2}-39\\lambda+1 \\end{eqnarray*}$$\n",
        "\n",
        "• Por lo que al encontrar las raíces de esta ecuación cuadrática, se tiene que$$\\begin{cases}\n",
        "\\lambda_{1}=\\frac{39+\\sqrt{1517}}{2} & =38.97434\\\\\n",
        "\\lambda_{2}=\\frac{39-\\sqrt{1517}}{2} & =0.02566\n",
        "\\end{cases}$$\n",
        "\n",
        "$$\\therefore\\left\\Vert A\\right\\Vert _{2}=\\sqrt{38.97434}=6.24294338$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vK3jux7k_4C7",
        "outputId": "53ae343f-c770-4334-d389-bbe03dcda1fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "A = np.array([[2, 5],[1, 3]])\n",
        "print (A)\n",
        "\n",
        "# Calculamos la 2-norma de A\n",
        "print (np.linalg.norm(A, 2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2 5]\n",
            " [1 3]]\n",
            "6.2429433838655335\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAh_UjLcaKr7"
      },
      "source": [
        "## Referencias\n",
        "\n",
        "*   Riswan Butt, Numerical Analysys Using Matlab, Jones and Bartlett.\n",
        "*   Ward Cheney, David Kincaid, Métodos Numéricos y Computación, Cenage Learning.\n",
        "*   Richard L. Burden, J. Douglas Faires, Análisis Numérico, Math Learning.\n",
        "*   Yuri N. Skiba, Introducción a los Métodos Numéricos. "
      ]
    }
  ]
}